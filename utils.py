"""
Helper functions for train, infer, and eval modules.
"""
import os
import re
import glob
import numpy as np
from gym_pcgrl import wrappers

from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv
import torch
import torch.nn.functional as F

import hashlib
import struct


def merge_npz_files(file_pattern):
    """
    Merges multiple npz files into a single npz file.

    Args:
        file_pattern (str): A glob-style pattern to match the npz files to be merged.
        output_filename (str): The name of the output npz file.
    """
    all_data = {}
    file_list = glob.glob(file_pattern)

    for filename in file_list:
        with np.load(filename) as data:
            for key in data:
                if key in all_data:
                    all_data[key] = np.concatenate((all_data[key], data[key]))
                else:
                    all_data[key] = data[key]

    # np.savez(output_filename, **all_data)
    return all_data


class RenderMonitor(Monitor):
    """
    Wrapper for the environment to save data in .csv files.
    """
    def __init__(self, env, rank, log_dir, **kwargs):
        self.log_dir = log_dir
        self.rank = rank
        self.render_gui = kwargs.get('render', False)
        self.render_rank = kwargs.get('render_rank', 0)
        if log_dir is not None:
            log_dir = os.path.join(log_dir, str(rank))
        Monitor.__init__(self, env, log_dir)

    def step(self, action):
        if self.render_gui and self.rank == self.render_rank:
            self.render()
        output = Monitor.step(self, action)
        import pdb; pdb.set_trace()
        return output
    

def get_action(obs, env, model, action_type=True):
    action = None
    if action_type == 0:
        action, _ = model.predict(obs)
    elif action_type == 1:
        action_prob = model.action_probability(obs)[0]
        action = np.random.choice(a=list(range(len(action_prob))), size=1, p=action_prob)
    else:
        action = np.array([env.action_space.sample()])
    return action


def get_exp_name(game, representation, experiment, **kwargs):
    exp_name = '{}_{}'.format(game, representation)
    if experiment is not None:
        exp_name = '{}_{}'.format(exp_name, experiment)
    return exp_name


def max_exp_idx(exp_name):
    log_dir = os.path.join("./runs", exp_name)
    log_files = glob.glob('{}*'.format(log_dir))
    if len(log_files) == 0:
        n = 0
    else:
        log_ns = [re.search('_(\d+)', f).group(1) for f in log_files]
        n = max(log_ns)
    return int(n)


def load_model(log_dir, full_path=None):
    if full_path:
        model = PPO2.load(full_path)
        return model

    model_path = os.path.join(log_dir, 'latest_model.pkl')
    if not os.path.exists(model_path):
        model_path = os.path.join(log_dir, 'latest_model.zip')
    if not os.path.exists(model_path):
        model_path = os.path.join(log_dir, 'best_model.pkl')
    if not os.path.exists(model_path):
        model_path = os.path.join(log_dir, 'best_model.zip')
    if not os.path.exists(model_path):
        files = [f for f in os.listdir(log_dir) if '.pkl' in f or '.zip' in f]
        if len(files) > 0:
            # selects the last file listed by os.listdir
            model_path = os.path.join(log_dir, np.random.choice(files))
        else:
            raise Exception('No models are saved')
    model = PPO2.load(model_path)
    return model


def _int_list_from_bigint(bigint):
    # Special case 0
    if bigint < 0:
        raise error.Error("Seed must be non-negative, not {}".format(bigint))
    elif bigint == 0:
        return [0]

    ints = []
    while bigint > 0:
        bigint, mod = divmod(bigint, 2**32)
        ints.append(mod)
    return ints


def get_string_map(map, tiles):
    int_to_string = dict((i, s) for i, s in enumerate(tiles))
    result = []
    for y in range(map.shape[0]):
        result.append([])
        for x in range(map.shape[1]):
            result[y].append(int_to_string[int(map[y][x])])
    return result
    
    
def _bigint_from_bytes(bytes):
    sizeof_int = 4
    padding = sizeof_int - len(bytes) % sizeof_int
    bytes += b"\0" * padding
    int_count = int(len(bytes) / sizeof_int)
    unpacked = struct.unpack("{}I".format(int_count), bytes)
    accum = 0
    for i, val in enumerate(unpacked):
        accum += 2 ** (sizeof_int * 8 * i) * val
    return accum
    

def create_seed(a=None, max_bytes=8):
    """Create a strong random seed. Otherwise, Python 2 would seed using
    the system time, which might be non-robust especially in the
    presence of concurrency.

    Args:
        a (Optional[int, str]): None seeds from an operating system specific randomness source.
        max_bytes: Maximum number of bytes to use in the seed.
    """
    # Adapted from https://svn.python.org/projects/python/tags/r32/Lib/random.py
    if a is None:
        a = _bigint_from_bytes(os.urandom(max_bytes))
    elif isinstance(a, str):
        a = a.encode("utf8")
        a += hashlib.sha512(a).digest()
        a = _bigint_from_bytes(a[:max_bytes])
    elif isinstance(a, int):
        a = a % 2 ** (8 * max_bytes)
    else:
        raise error.Error("Invalid type for seed: {} ({})".format(type(a), a))

    return a

    
def hash_seed(seed=None, max_bytes=8):
    """Any given evaluation is likely to have many PRNG's active at
    once. (Most commonly, because the environment is running in
    multiple processes.) There's literature indicating that having
    linear correlations between seeds of multiple PRNG's can correlate
    the outputs:

    http://blogs.unity3d.com/2015/01/07/a-primer-on-repeatable-random-numbers/
    http://stackoverflow.com/questions/1554958/how-different-do-random-seeds-need-to-be
    http://dl.acm.org/citation.cfm?id=1276928

    Thus, for sanity we hash the seeds before using them. (This scheme
    is likely not crypto-strength, but it should be good enough to get
    rid of simple correlations.)

    Args:
        seed (Optional[int]): None seeds from an operating system specific randomness source.
        max_bytes: Maximum number of bytes to use in the hashed seed.
    """
    if seed is None:
        seed = create_seed(max_bytes=max_bytes)
    hash = hashlib.sha512(str(seed).encode("utf8")).digest()
    return _bigint_from_bytes(hash[:max_bytes])
    

def np_random(seed=None):
    if seed is not None and not (isinstance(seed, int) and 0 <= seed):
        raise error.Error(
            "Seed must be a non-negative integer or omitted, not {}".format(seed)
        )

    seed = create_seed(seed)

    rng = np.random.RandomState()
    rng.seed(_int_list_from_bigint(hash_seed(seed)))
    return rng, seed
    

def int_map_from_onehot(map, dims=21):
    int_map = []
    for y in map:
        for x in y:
            vals = np.argmax(x, axis=1)
            # print(f"vals: {vals}")
            int_map.extend(list(vals))

    return np.array(int_map).reshape((dims,dims))
        

def get_action(obs, env, model, action_type=True):
    action = None
    if action_type == 0:
        action, _ = model.predict(obs)
    elif action_type == 1:
        action_prob = model.action_probability(obs)[0]
        action = np.random.choice(a=list(range(len(action_prob))), size=1, p=action_prob)
    else:
        action = np.array([env.action_space.sample()])
    return action

def make_env(env_name, representation, rank=0, log_dir=None, **kwargs):
    '''
    Return a function that will initialize the environment when called.
    '''
    max_step = kwargs.get('max_step', None)
    render = kwargs.get('render', False)
    def _thunk():
        if representation == 'wide':
            # print(f"kwargs is {kwargs}")
            env = wrappers.ActionMapImagePCGRLWrapper(env_name, **kwargs)
        else:
            crop_size = kwargs.get('cropped_size', 22)
            env = wrappers.CroppedImagePCGRLWrapper(env_name, crop_size, **kwargs)
            # print(f"kwargs is {kwargs}")
        # RenderMonitor must come last
        if render or log_dir is not None and len(log_dir) > 0:
            env = RenderMonitor(env, rank, log_dir, **kwargs)
        return env
    return _thunk

def make_vec_envs(env_name, representation, log_dir, n_cpu, **kwargs):
    '''
    Prepare a vectorized environment using a list of 'make_env' functions.
    '''
    if n_cpu > 1:
        env_lst = []
        for i in range(n_cpu):
            env_lst.append(make_env(env_name, representation, i, log_dir, **kwargs))
        env = SubprocVecEnv(env_lst)
    else:
        env = DummyVecEnv([make_env(env_name, representation, 0, log_dir, **kwargs)])
    return env


def get_exp_name(game, representation, experiment, **kwargs):
    exp_name = '{}_{}'.format(game, representation)
    if experiment is not None:
        exp_name = '{}_{}'.format(exp_name, experiment)
    return exp_name


def max_exp_idx(exp_name):
    log_dir = os.path.join("./runs", exp_name)
    log_files = glob.glob('{}*'.format(log_dir))
    if len(log_files) == 0:
        n = 0
    else:
        log_ns = [re.search('_(\d+)', f).group(1) for f in log_files]
        n = max(log_ns)
    return int(n)


def calculate_entropy(logits):
    """
    Calculate the entropy of the predicted distribution from logits.
    
    Args:
        logits: Output logits from the construction network
        
    Returns:
        Entropy value
    """
    # Apply softmax to convert logits to probabilities
    probabilities = F.softmax(logits, dim=1)
    
    # Calculate entropy: -sum(p * log(p))
    # Add small epsilon to avoid log(0)
    epsilon = 1e-10
    entropy = -torch.sum(probabilities * torch.log(probabilities + epsilon), dim=1)
    
    return entropy

def evaluate_solvability(state):
    """
    Evaluate how solvable the current state is.
    This is a placeholder function - in a real implementation, you would use
    domain-specific logic based on your PCGRL environment.
    
    Args:
        state: The current grid state
        
    Returns:
        Solvability score (0-100)
    """
    # Placeholder implementation - in a real scenario, you would:
    # 1. Check if the level is structurally valid
    # 2. Check if game-specific rules are satisfied (paths exist, etc.)
    # 3. Run a solver to check if the level is completable
    
    # For demonstration, let's assume a simple metric:
    # Higher percentage of valid tiles = higher solvability
    
    # Convert from one-hot to tile indices
    if len(state.shape) > 2 and state.shape[-1] > 1:  # one-hot encoded
        tile_indices = np.argmax(state, axis=-1)
    else:  # already indices
        tile_indices = state
    
    # Example: Assume tiles 0 and 1 are "valid" and others are "invalid"
    valid_tiles = (tile_indices == 0) | (tile_indices == 1)
    valid_percentage = np.mean(valid_tiles) * 100
    
    # Add some randomness for demonstration purposes
    noise = np.random.normal(0, 5)  # Mean 0, std 5
    solvability = np.clip(valid_percentage + noise, 0, 100)
    
    return solvability

def evaluate_diversity(state):
    """
    Evaluate how diverse or interesting the current state is.
    This is a placeholder function - in a real implementation, you would use
    domain-specific metrics based on your PCGRL environment.
    
    Args:
        state: The current grid state
        
    Returns:
        Diversity score (0-100)
    """
    # Placeholder implementation - in a real scenario, you would:
    # 1. Measure pattern variety 
    # 2. Count different region types
    # 3. Assess structural complexity
    
    # For demonstration, let's assume a simple metric:
    # Higher entropy of tile distribution = higher diversity
    
    # Convert from one-hot to tile indices
    if len(state.shape) > 2 and state.shape[-1] > 1:  # one-hot encoded
        tile_indices = np.argmax(state, axis=-1)
    else:  # already indices
        tile_indices = state
    
    # Count occurrences of each tile type
    unique, counts = np.unique(tile_indices, return_counts=True)
    total_tiles = np.sum(counts)
    
    # Calculate tile distribution entropy
    probabilities = counts / total_tiles
    entropy = -np.sum(probabilities * np.log2(probabilities + 1e-10))
    
    # Normalize to 0-100 scale
    # Assuming max possible entropy is log2(num_tile_types)
    num_tile_types = len(unique)
    max_entropy = np.log2(num_tile_types)
    normalized_entropy = (entropy / max_entropy) * 100
    
    # Add some randomness for demonstration purposes
    noise = np.random.normal(0, 5)  # Mean 0, std 5
    diversity = np.clip(normalized_entropy + noise, 0, 100)
    
    return diversity